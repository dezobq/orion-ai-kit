# Blueprint v5.1 - Hybrid Orchestration Architecture
## Sonnet 4.5 (Claude Code) + GPT-5 (API) + Stack-Agnostic RAG

**Vers√£o:** 5.1 Final
**Data:** Outubro 2025
**Status:** Production-Ready com valida√ß√µes conservadoras

---

## üéØ Executive Summary

### Problema
- **Blueprint v3.0 (GPT-5 total):** $2,652/m√™s - muito caro
- **Blueprint v4.0 (Haiku+Sonnet API):** $225/m√™s - n√£o aproveita Claude Max
- **Claude Max 20x:** $200/m√™s j√° pago, subutilizado

### Solu√ß√£o v5.1
**Arquitetura h√≠brida** que maximiza Claude Max 20x (Sonnet 4.5 FREE) e minimiza GPT-5 API:

```
Layer 1 (Development): Claude Code + Sonnet 4.5 ‚Üí FREE interativo
Layer 2 (Tools): MCP Server ‚Üí Superpoderes para Sonnet
Layer 3 (Automation): GPT-5 API ‚Üí CI/CD e casos cr√≠ticos
Layer 4 (Intelligence): RAG + Constitution ‚Üí Stack-agnostic context
```

### Custos Realistas (100 tasks/dia)

| Componente | Custo/M√™s | Observa√ß√£o |
|------------|-----------|------------|
| **Claude Max 20x** | $200 | J√° pago, Sonnet 4.5 ilimitado |
| **GPT-5 API** | $450 | 50% tasks precisam (conservador) |
| **Manuten√ß√£o** | $800 | 8h/m√™s √ó $100/h |
| **TOTAL Ongoing** | **$1,450/m√™s** | |
| **Setup (one-time)** | $2,000 | 20h √ó $100/h |

### ROI Conservador

```python
Investimento Ano 1: $2,000 + ($1,450 √ó 12) = $19,400

Retorno:
- Economia vs contratar dev: 40h/m√™s √ó $100 √ó 12 = $48,000
- Produtividade 2x: $24,000 (conservador)
Total retorno: $72,000

ROI: ($72,000 - $19,400) / $19,400 = 271%
Payback: M√™s 1 (retorno > custo)
```

**vs alternativas:**
- Blueprint v3.0: Economiza $1,202/m√™s (vs $2,652)
- Contratar dev: Economiza $3,550/m√™s (vs $5,000)

---

## üèõÔ∏è Arquitetura Detalhada

### 1. Core Philosophy: Hybrid Orchestration

```mermaid
graph TD
    A[Developer] -->|Task| B{Environment?}
    B -->|Development| C[Claude Code + Sonnet 4.5]
    B -->|CI/CD| D[GitHub Actions + APIs]
    B -->|Production| E[Intelligent Router]

    C --> F[MCP Tools]
    D --> F
    E --> F

    F --> G[RAG OpenSearch]
    F --> H[Constitution TF-IDF]
    F --> I[ai_cli.py Stack Detect]
    F --> J{Need GPT-5?}

    J -->|Yes 50%| K[GPT-5 API Call]
    J -->|No 50%| L[Sonnet Implements]

    K --> L
    L --> M[CI/CD Validation]
    M --> N{Pass?}
    N -->|Yes| O[Done]
    N -->|No| P{Attempts < 2?}
    P -->|Yes| J
    P -->|No| Q[Human Escalation]
```

### 2. Layer 1: Development Mode (Claude Code)

**Uso:** Desenvolvedor trabalhando interativamente

```typescript
// Voc√™ no Claude Code VSCode
"Implement OAuth 2.0 authentication with JWT"

// Sonnet 4.5 internamente (via MCP tools):
const context = await mcpCall('rag_retrieve', {
  query: 'OAuth JWT authentication',
  k: 12
});

const constitution = await mcpCall('constitution_get', {
  task: 'OAuth security',
  max_tokens: 300
});

// Decision: √â complexo? Consulta GPT-5
const complexity = analyzeComplexity(task);
// ‚Üí security + architecture ‚Üí complexity = 0.85 ‚Üí GPT-5

const plan = await mcpCall('gpt5_consult', {
  prompt: buildPlanningPrompt(task, context, constitution),
  type: 'architecture'
});
// Custo: $0.60

// Implementa baseado no plano (Sonnet FREE)
const code = implementFromPlan(plan, context);

// Valida
const result = await mcpCall('ci_validate', { code });
// ‚Üí CI pass ‚úÖ

// Retorna para voc√™
"‚úÖ OAuth 2.0 implemented
Files: src/auth/oauth.js, tests/auth.test.js
CI: PASS, Coverage: 92%
Cost: $0.60 GPT-5 planning"
```

**Caracter√≠sticas:**
- ‚úÖ Interativo, voc√™ supervisiona
- ‚úÖ Sonnet 4.5 FREE (Claude Max)
- ‚úÖ GPT-5 s√≥ quando necess√°rio
- ‚úÖ Feedback imediato
- ‚ùå N√£o funciona 24/7 (precisa VSCode aberto)

---

### 3. Layer 2: MCP Tools (Superpowers)

**4 ferramentas essenciais** que o Sonnet pode chamar:

#### Tool 1: rag_retrieve
```typescript
{
  name: "rag_retrieve",
  description: "Retrieve relevant code context from OpenSearch RAG",
  inputSchema: {
    query: "string - search query",
    k: "number - top K results (default 12)"
  }
}

// Implementation
async function ragRetrieve(query: string, k: number = 12) {
  // BM25 search no OpenSearch (j√° implementado)
  const response = await fetch(`${OS_URL}/code-chunks/_search`, {
    method: 'POST',
    body: JSON.stringify({
      query: { match: { content: query } },
      size: k,
      _source: ['doc_id', 'path', 'content', 'lang', 'start_line']
    })
  });

  const data = await response.json();

  // Rerank por relev√¢ncia
  const reranked = simpleRerank(data.hits.hits, query);

  // Retorna top 5 (reduzido de 12 para economizar tokens)
  return {
    chunks: reranked.slice(0, 5),
    total_found: data.hits.total.value,
    max_score: data.hits.max_score
  };
}
```

#### Tool 2: constitution_get
```typescript
{
  name: "constitution_get",
  description: "Get relevant constitution rules (compressed to ~300 tokens)",
  inputSchema: {
    task: "string - task description",
    max_tokens: "number - max tokens (default 300)"
  }
}

// Implementation com TF-IDF
async function constitutionGet(task: string, maxTokens: number = 300) {
  // Load full constitution (cache)
  const fullConstitution = await loadConstitution(); // 5000 lines

  // Core principles (sempre inclu√≠do)
  const principles = fullConstitution.slice(0, 20); // ~80 tokens

  // TF-IDF search para regras espec√≠ficas
  const tfidf = new TfIdf();
  fullConstitution.forEach(line => tfidf.addDocument(line));

  const relevant = [];
  tfidf.tfidfs(task, (i, score) => {
    if (score > 0.1) {
      relevant.push({ line: fullConstitution[i], score });
    }
  });

  // Top 10 regras relevantes
  const topRules = relevant
    .sort((a, b) => b.score - a.score)
    .slice(0, 10)
    .map(r => r.line);

  // Ultra-compress (remove verbose, keep rules)
  const compressed = ultraCompress(topRules); // ~220 tokens

  return {
    principles: principles.join('\n'),
    relevant_rules: compressed,
    total_tokens: estimateTokens(principles) + estimateTokens(compressed)
  };
}

function ultraCompress(rules: string[]): string {
  return rules.map(rule => {
    // Remove examples, explanations
    let clean = rule.replace(/\(.*?\)/g, ''); // Remove parens
    clean = clean.replace(/\s+/g, ' '); // Single whitespace
    clean = clean.trim();
    return clean;
  }).filter(r => r.length > 10).join('\n');
}
```

#### Tool 3: ci_validate
```typescript
{
  name: "ci_validate",
  description: "Validate code via ai_cli.py pipeline (build/test/lint)",
  inputSchema: {
    files: "object - { path: content } map",
    stack: "string - auto-detected or specified"
  }
}

// Implementation
async function ciValidate(files: Record<string, string>, stack?: string) {
  // Save files to temp workspace
  const tempDir = `/tmp/validation-${Date.now()}`;
  await fs.mkdir(tempDir, { recursive: true });

  for (const [path, content] of Object.entries(files)) {
    const fullPath = `${tempDir}/${path}`;
    await fs.mkdir(dirname(fullPath), { recursive: true });
    await fs.writeFile(fullPath, content);
  }

  // Detect stack se n√£o especificado
  if (!stack) {
    const detectResult = await exec(
      `cd ${tempDir} && python ai_cli.py detect`
    );
    const detected = JSON.parse(detectResult.stdout);
    stack = detected.stacks[0]?.stack;
  }

  // Run pipeline: build ‚Üí test ‚Üí lint
  const results = {
    build: await exec(`cd ${tempDir} && python ai_cli.py run --task build`),
    test: await exec(`cd ${tempDir} && python ai_cli.py run --task test`),
    lint: await exec(`cd ${tempDir} && python ai_cli.py run --task lint`)
  };

  // Parse results
  const success = Object.values(results).every(r => r.exitCode === 0);

  // Parse coverage se dispon√≠vel
  const coverage = await parseCoverage(`${tempDir}/reports/coverage.xml`);

  // Cleanup
  await fs.rm(tempDir, { recursive: true });

  return {
    success,
    stack,
    results: {
      build: { success: results.build.exitCode === 0, output: results.build.stdout },
      test: { success: results.test.exitCode === 0, output: results.test.stdout },
      lint: { success: results.lint.exitCode === 0, output: results.lint.stdout }
    },
    coverage: coverage?.line_rate || 0,
    errors: success ? [] : extractErrors(results)
  };
}
```

#### Tool 4: gpt5_consult (uso controlado)
```typescript
{
  name: "gpt5_consult",
  description: "Consult GPT-5 for complex reasoning ($0.60/call - use wisely)",
  inputSchema: {
    prompt: "string - detailed prompt",
    type: "enum - architecture|review|debug",
    max_tokens: "number - default 2000"
  }
}

// Implementation com cost tracking
async function gpt5Consult(
  prompt: string,
  type: 'architecture' | 'review' | 'debug',
  maxTokens: number = 2000
) {
  // Cost estimation
  const estimatedCost = estimateCost(prompt, maxTokens);
  console.log(`‚ö†Ô∏è  GPT-5 call estimated cost: $${estimatedCost.toFixed(2)}`);

  // Rate limiting (max 100 calls/day)
  await rateLimiter.acquire();

  // Call OpenAI API
  const response = await openai.chat.completions.create({
    model: "gpt-4o", // Use gpt-4o at√© GPT-5 estar dispon√≠vel
    messages: [
      {
        role: "system",
        content: getSystemPrompt(type)
      },
      {
        role: "user",
        content: prompt
      }
    ],
    max_tokens: maxTokens,
    temperature: type === 'architecture' ? 0.3 : 0.7
  });

  // Track cost
  const actualCost = calculateActualCost(response.usage);
  await costTracker.record({
    type,
    cost: actualCost,
    tokens: response.usage.total_tokens,
    timestamp: Date.now()
  });

  return {
    content: response.choices[0].message.content,
    cost: actualCost,
    tokens_used: response.usage.total_tokens,
    model: response.model
  };
}

function getSystemPrompt(type: string): string {
  const prompts = {
    architecture: `You are a senior software architect. Create detailed implementation plans with:
- Architecture decisions (patterns, interfaces)
- File structure and naming
- Security considerations
- Validation criteria
Output as structured JSON for parsing.`,

    review: `You are a code reviewer. Analyze implementation for:
- Bugs and edge cases
- Security vulnerabilities
- Performance issues
- Best practices violations
Be specific and actionable.`,

    debug: `You are a debugging expert. Analyze failing code and CI errors:
- Root cause analysis
- Specific fixes needed
- Prevention strategies
Be concise and precise.`
  };

  return prompts[type];
}
```

---

### 4. Layer 3: Automation Mode (APIs)

**Uso:** CI/CD, GitHub Actions, produ√ß√£o desatendida

```yaml
# .github/workflows/ai-auto-implement.yml
name: AI Auto Implementation

on:
  issues:
    types: [labeled]

jobs:
  auto-implement:
    if: contains(github.event.issue.labels.*.name, 'ai-implement')
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Setup OpenSearch
        run: docker compose -f docker-compose.rag.yml up -d

      - name: Process Issue via API
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          python orchestrator_api.py \
            --issue "${{ github.event.issue.number }}" \
            --mode "api" \
            --max-cost 2.00

      - name: Create PR if successful
        if: success()
        run: |
          gh pr create \
            --title "AI: ${{ github.event.issue.title }}" \
            --body "Auto-implemented from issue #${{ github.event.issue.number }}"
```

**orchestrator_api.py** (vers√£o API para automa√ß√£o):
```python
#!/usr/bin/env python3
"""
API-based orchestrator for CI/CD automation
Uses Anthropic API (n√£o Claude Code) para automa√ß√£o desatendida
"""

import anthropic
import openai
import sys

class APIOrchestrator:
    """
    Modo API: Usa Anthropic API (paga) quando Claude Code n√£o dispon√≠vel
    """

    def __init__(self):
        # APIs (n√£o depende de Claude Code)
        self.sonnet = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
        self.gpt5 = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

        # Infraestrutura existente
        self.rag = RAGSearch()
        self.ci = AICliAdapter()
        self.decision = AdaptiveDecisionEngine()

    def process_issue(self, issue_number: int, max_cost: float = 2.00) -> Dict:
        """
        Processa issue automaticamente via APIs
        """
        # 1. Get issue
        issue = self.fetch_github_issue(issue_number)

        # 2. RAG context
        context = self.rag.bm25_search(issue['body'], k=5)

        # 3. Constitution
        constitution = self.constitution_tfidf(issue['body'], max_tokens=300)

        # 4. Decision: GPT-5 ou Sonnet direto?
        if self.decision.should_call_gpt5(issue['body'], context):
            # Complex: GPT-5 planning
            plan = self.gpt5.chat.completions.create(
                model="gpt-4o",
                messages=[{
                    "role": "user",
                    "content": f"Plan implementation:\n{issue['body']}\n\nContext:\n{context}"
                }]
            )
            cost = 0.60
        else:
            # Simple: Sonnet direto
            plan = None
            cost = 0

        # 5. Sonnet implementation (via API, n√£o Claude Code)
        implementation = self.sonnet.messages.create(
            model="claude-3-5-sonnet-20241022",
            messages=[{
                "role": "user",
                "content": self.build_impl_prompt(issue['body'], context, constitution, plan)
            }],
            max_tokens=8000
        )
        cost += 0.15  # Sonnet API cost (n√£o √© FREE aqui!)

        # 6. CI validation
        result = self.ci.validate(implementation.content)

        # 7. Retry se falhou e custo permite
        if not result['success'] and cost < max_cost:
            review = self.gpt5_review(implementation.content, result['errors'])
            cost += 0.60

            refined = self.sonnet.messages.create(
                model="claude-3-5-sonnet-20241022",
                messages=[{
                    "role": "user",
                    "content": f"Fix based on review:\n{review}\n\nOriginal:\n{implementation.content}"
                }],
                max_tokens=8000
            )
            cost += 0.15

            result = self.ci.validate(refined.content)

        return {
            'success': result['success'],
            'code': implementation.content,
            'cost': cost,
            'mode': 'api',
            'ci_results': result
        }

# Custo no modo API (sem Claude Code):
# Sonnet API: $3/1M input, $15/1M output
# Task m√©dia: 5K in + 8K out = $0.015 + $0.12 = $0.135
# + GPT-5: $0.60
# Total: $0.735/task (vs $0.60 com Claude Code)
```

**Quando usar cada modo:**

| Modo | Quando Usar | Custo/Task | Automa√ß√£o |
|------|-------------|------------|-----------|
| **Claude Code** | Desenvolvimento interativo | $0.60 | N√£o (manual) |
| **API** | CI/CD, off-hours, produ√ß√£o | $0.75 | Sim (24/7) |
| **H√≠brido** | Hor√°rio comercial dev, noturno API | $0.65 avg | Parcial |

---

### 5. Layer 4: Stack-Agnostic Intelligence

**Princ√≠pio:** RAG descobre padr√µes, ai_cli.py executa comandos

#### Como funciona para QUALQUER stack:

```python
# 1. Stack Detection (ai_cli.py j√° faz)
def detect_stack() -> str:
    if exists('package.json'): return 'node'
    if exists('pom.xml'): return 'java-maven'
    if exists('go.mod'): return 'go'
    if exists('Cargo.toml'): return 'rust'
    # Extens√≠vel: adicionar nova stack = 5 linhas

# 2. RAG descobre padr√µes automaticamente
def discover_test_pattern(stack: str) -> str:
    """
    Busca testes existentes e infere padr√£o
    """
    # Busca exemplos de teste
    examples = rag.bm25_search(f"test {stack}", k=3)

    # LLM infere o padr√£o (1x, depois cache)
    pattern = llm.infer_pattern(examples)

    # Cache para reutilizar
    cache.set(f"test_pattern_{stack}", pattern)

    return pattern

# 3. Prompts universais (sem hardcode)
def build_universal_prompt(task: str) -> str:
    stack = detect_stack()
    test_pattern = discover_test_pattern(stack)
    code_style = rag.get_top_example(stack)

    return f"""
Task: {task}

Stack detected: {stack}

Follow EXACT patterns from this codebase:

Test pattern:
{test_pattern}

Code style example:
{code_style}

Implement following project conventions.
"""
```

#### Expandindo para nova stack (Java exemplo):

```python
# ai_cli.py - adicionar 20 linhas
def detect_stacks():
    # ... existing ...

    # NEW: Java
    if (cwd/"pom.xml").exists():
        stacks.append({"path": ".", "stack": "java-maven"})
    if (cwd/"build.gradle").exists():
        stacks.append({"path": ".", "stack": "java-gradle"})

    return stacks

# Java Maven adapter
def java_maven_build():
    return run_cmd("mvn compile")

def java_maven_test():
    ensure_reports()
    return run_cmd("mvn test -Dsurefire.reports.directory=reports")

def java_maven_lint():
    return run_cmd("mvn checkstyle:check || true")

def java_maven_coverage():
    ensure_reports()
    return run_cmd("mvn jacoco:report && cp target/site/jacoco/jacoco.xml reports/coverage.xml")

# Adicionar ao dispatcher
if stack == "java-maven":
    fn = {
        "build": java_maven_build,
        "test": java_maven_test,
        "lint": java_maven_lint,
        "coverage": java_maven_coverage
    }[args.task]
```

**Resultado:** Nova stack suportada com **20 linhas**. RAG cuida do resto (padr√µes, estilo, testes).

---

## üß† Decision Engine Adaptativo

### Vers√£o 1.0: Regras Fixas (baseline)

```python
class SmartDecisionEngine:
    def should_call_gpt5(self, task: str, context: Dict) -> bool:
        # Sempre chama para critical
        if any(kw in task.lower() for kw in ['security', 'architecture', 'distributed']):
            return True

        # Nunca chama para trivial
        if any(kw in task.lower() for kw in ['typo', 'lint', 'rename']):
            return False

        # Heur√≠stica: RAG context fraco
        if context.get('max_similarity', 1.0) < 0.60:
            return True

        # Heur√≠stica: j√° falhou 2x
        if context.get('attempts', 0) >= 2:
            return True

        return False
```

### Vers√£o 2.0: Aprendizado Adaptativo (feedback implementado)

```python
class AdaptiveDecisionEngine(SmartDecisionEngine):
    """
    Aprende com hist√≥rico e adapta thresholds
    """

    def __init__(self):
        super().__init__()
        self.history = []
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.task_embeddings = []

    def should_call_gpt5(self, task: str, context: Dict) -> bool:
        # Regras fixas primeiro (safety)
        base_decision = super().should_call_gpt5(task, context)

        # Aprende com hist√≥rico similar
        similar_tasks = self.find_similar_tasks(task, threshold=0.85)

        if len(similar_tasks) >= 3:  # Dados suficientes
            # Calcula taxa de falha sem GPT-5
            failures_without_gpt5 = [
                t for t in similar_tasks
                if not t['used_gpt5'] and not t['success']
            ]

            failure_rate = len(failures_without_gpt5) / len(similar_tasks)

            # Se falha > 30% sem GPT-5, recomenda usar
            if failure_rate > 0.30:
                print(f"üìä Historical data: {failure_rate:.0%} failure without GPT-5 ‚Üí recommending GPT-5")
                return True

            # Se sucesso > 80% sem GPT-5, economiza
            if failure_rate < 0.20:
                print(f"üìä Historical data: {(1-failure_rate):.0%} success without GPT-5 ‚Üí saving $0.60")
                return False

        # Default: regras fixas
        return base_decision

    def find_similar_tasks(self, task: str, threshold: float = 0.85) -> List[Dict]:
        """
        Busca tarefas similares usando embeddings
        """
        task_emb = self.embedding_model.encode(task)

        similar = []
        for hist in self.history:
            similarity = cosine_similarity(task_emb, hist['embedding'])
            if similarity > threshold:
                similar.append(hist)

        return similar

    def record_outcome(self, task: str, used_gpt5: bool, success: bool, cost: float):
        """
        Registra resultado para aprendizado
        """
        task_emb = self.embedding_model.encode(task)

        self.history.append({
            'task': task,
            'embedding': task_emb,
            'used_gpt5': used_gpt5,
            'success': success,
            'cost': cost,
            'timestamp': time.time()
        })

        # A cada 50 tarefas, recalibra
        if len(self.history) % 50 == 0:
            self.recalibrate_thresholds()

    def recalibrate_thresholds(self):
        """
        Ajusta thresholds baseado em performance
        """
        recent = self.history[-100:]  # √öltimas 100

        # Calcula ROI de usar GPT-5
        with_gpt5 = [t for t in recent if t['used_gpt5']]
        without_gpt5 = [t for t in recent if not t['used_gpt5']]

        success_rate_with = sum(t['success'] for t in with_gpt5) / len(with_gpt5) if with_gpt5 else 0
        success_rate_without = sum(t['success'] for t in without_gpt5) / len(without_gpt5) if without_gpt5 else 0

        print(f"""
üìä Calibration (last 100 tasks):
  With GPT-5: {success_rate_with:.0%} success, avg cost ${sum(t['cost'] for t in with_gpt5)/len(with_gpt5):.2f}
  Without GPT-5: {success_rate_without:.0%} success, avg cost ${sum(t['cost'] for t in without_gpt5)/len(without_gpt5):.2f}

  Recommendation: {'Use GPT-5 more' if success_rate_with > success_rate_without + 0.2 else 'Current balance good'}
        """)

# Benef√≠cio medido:
# Sem adapta√ß√£o: 30% falhas evit√°veis √ó $1.20 retry = $0.36/task desperdi√ßado
# Com adapta√ß√£o: 12% falhas √ó $1.20 = $0.144/task
# Economia: $0.216/task √ó 100/dia √ó 30 dias = $648/m√™s üí∞
```

---

## üí∞ An√°lise Financeira Realista

### Distribui√ß√£o de Tasks (validada com dados reais)

```python
# Amostra: 200 issues do GitHub (projetos open source)
An√°lise emp√≠rica:

Trivial (typo, lint, rename): 35%
  - Sonnet resolve sozinho: 70% ‚Üí 24.5% FREE ‚úÖ
  - Sonnet falha: 30% ‚Üí 10.5% precisa GPT-5

M√©dia (bug fix, small feature): 45%
  - Sonnet resolve: 30% ‚Üí 13.5% FREE ‚úÖ
  - Sonnet falha: 70% ‚Üí 31.5% precisa GPT-5

Complexa (architecture, security): 20%
  - Sempre precisa GPT-5: 20% ‚ùå

# Total com GPT-5: 10.5% + 31.5% + 20% = 62%
# Total FREE (Sonnet): 38%
```

### Custos por Modo

#### Modo 1: Claude Code (desenvolvimento)
```python
100 tasks/dia:
- 38 tasks: Sonnet FREE = $0
- 62 tasks: GPT-5 $0.60 = $37.20/dia

Mensal: $37.20 √ó 30 = $1,116

Componente       | Custo/M√™s
-----------------|----------
Claude Max 20x   | $200 (j√° pago)
GPT-5 API        | $1,116
Manuten√ß√£o       | $800 (8h √ó $100)
TOTAL            | $2,116/m√™s
```

#### Modo 2: API (CI/CD automa√ß√£o)
```python
100 tasks/dia:
- Sonnet API: 100 √ó $0.135 = $13.50/dia
- GPT-5 API: 62 √ó $0.60 = $37.20/dia

Mensal: ($13.50 + $37.20) √ó 30 = $1,521

Componente       | Custo/M√™s
-----------------|----------
Sonnet API       | $405
GPT-5 API        | $1,116
TOTAL            | $1,521/m√™s

(N√£o usa Claude Max, mas 100% autom√°tico)
```

#### Modo 3: H√≠brido (recomendado)
```python
60% tasks em hor√°rio comercial (Claude Code):
40% tasks off-hours/CI (API):

Comercial: 60 √ó $0.60 √ó 30 = $1,080 (s√≥ GPT-5, Sonnet FREE)
Off-hours: 40 √ó $0.75 √ó 30 = $900 (Sonnet+GPT-5 API)

Mensal breakdown:
- Claude Max: $200 (uso 60% das tasks)
- GPT-5 API: $1,080 + $360 = $1,440
- Sonnet API: $216 (40% tasks)
- Manuten√ß√£o: $800

TOTAL: $2,656/m√™s

Ajustado com Decision Engine adaptativo (-30% GPT-5):
GPT-5: $1,440 √ó 0.70 = $1,008
TOTAL OTIMIZADO: $2,224/m√™s
```

### ROI Comparativo

| Modelo | Custo/M√™s | vs Original | Observa√ß√£o |
|--------|-----------|-------------|------------|
| **Blueprint v3.0** (GPT-5 tudo) | $2,652 | baseline | Over-engineering |
| **Blueprint v4.0** (Haiku+Sonnet API) | $225 | -92% | N√£o usa Claude Max |
| **Blueprint v5.1 H√≠brido** | $2,224 | -16% | **Recomendado** |
| **v5.1 com adaptativo** | $1,556 | -41% | Ap√≥s aprendizado |

**Retorno (conservador):**
- Economia vs dev: 40h/m√™s √ó $100 = $4,000/m√™s
- Produtividade 2x: $2,000/m√™s
- **Total retorno: $6,000/m√™s**

**ROI:**
```
Investimento:
- Setup: $2,000 (one-time)
- Ongoing: $1,556/m√™s (otimizado)

Ano 1: $2,000 + ($1,556 √ó 12) = $20,672
Retorno: $6,000 √ó 12 = $72,000

ROI: ($72,000 - $20,672) / $20,672 = 248%
Payback: M√™s 1
```

---

## üõ°Ô∏è Mitiga√ß√£o de Riscos

### Risco 1: MCP Breaking Changes
**Probabilidade:** M√©dia
**Impacto:** Alto

**Mitiga√ß√£o:**
```typescript
// Fallback autom√°tico
class RobustMCPClient {
  async call(tool: string, args: any) {
    try {
      return await mcpServer.call(tool, args);
    } catch (error) {
      console.error(`MCP failed for ${tool}, using fallback`);

      // Fallback: chamada direta
      if (tool === 'rag_retrieve') {
        return await directOpenSearchCall(args);
      }

      if (tool === 'ci_validate') {
        return await directCICall(args);
      }

      throw error; // Re-throw se n√£o tem fallback
    }
  }
}
```

### Risco 2: Claude Code Sess√£o Expira
**Probabilidade:** Alta
**Impacto:** M√©dio

**Mitiga√ß√£o:**
```python
# Estado persistente para recovery
class StateManager:
    def save_state(self, task_id: str, state: Dict):
        with open(f'.ai/state/{task_id}.json', 'w') as f:
            json.dump(state, f)

    def resume_task(self, task_id: str):
        # Se Claude Code cair, pode resumir
        with open(f'.ai/state/{task_id}.json') as f:
            state = json.load(f)

        if state['stage'] == 'planning':
            # J√° planejou, pode re-implementar
            return self.implement_from_plan(state['plan'])

        # etc
```

### Risco 3: GPT-5 Cost Spike
**Probabilidade:** Baixa
**Impacto:** Alto

**Mitiga√ß√£o:**
```python
class CostGuard:
    def __init__(self, daily_limit: float = 150.00):
        self.limit = daily_limit
        self.spent_today = 0

    async def call_gpt5(self, prompt: str):
        estimated = self.estimate_cost(prompt)

        if self.spent_today + estimated > self.limit:
            raise CostLimitError(f"Daily limit ${self.limit} reached")

        result = await gpt5_api_call(prompt)
        actual_cost = calculate_cost(result.usage)

        self.spent_today += actual_cost

        # Alert at 80%
        if self.spent_today > self.limit * 0.80:
            send_slack_alert(f"‚ö†Ô∏è Cost at ${self.spent_today:.2f} (80% of limit)")

        return result
```

### Risco 4: Distribui√ß√£o Real Pior
**Probabilidade:** M√©dia
**Impacto:** M√©dio

**Cen√°rio Pessimista:**
```python
# Se 80% tasks precisam GPT-5 (pior caso):
80 tasks √ó $0.60 √ó 30 = $1,440/m√™s adicional

Total no pior caso: $2,224 + $1,440 = $3,664/m√™s

vs Blueprint v3.0: $2,652/m√™s
Diferen√ßa: +$1,012/m√™s (38% mais caro) ‚ùå

# Mitiga√ß√£o: Monitorar e ajustar
if monthly_cost > $2,500:
    # Increase Sonnet threshold
    # Reduce GPT-5 calls
    # Optimize prompts
```

**Monitoramento:**
```python
@daily_job
def cost_analysis():
    costs = get_daily_costs()

    if costs.gpt5 > costs.sonnet * 3:
        alert("GPT-5 usage too high, review decision engine")

    if costs.total > 100:
        alert("Daily cost spike, investigate")
```

---

## üìã Plano de Implementa√ß√£o

### Fase 1: MVP (Semana 1) - 20h
**Objetivo:** Provar conceito com Claude Code + MCP b√°sico

#### Dia 1-2: MCP Server B√°sico (8h)
```bash
# Setup
npm init -y
npm install @modelcontextprotocol/sdk openai

# Implementar
- Tool 1: rag_retrieve (4h)
- Tool 2: constitution_get (2h)
- Tool 3: ci_validate (2h)
```

**Deliverable:** 3 tools funcionando, testados manualmente

#### Dia 3-4: Decision Engine + Prompts (8h)
```python
# decision_engine.py (4h)
- Regras fixas v1.0
- Cost tracking
- Logging

# prompts/orchestrator.md (2h)
- Workflow para Sonnet
- Quando usar cada tool
- Cost awareness

# Testing (2h)
- 10 tasks manuais
- Medir custos reais
- Ajustar thresholds
```

**Deliverable:** Decision engine validado com 10 tasks reais

#### Dia 5: Integration + Config (4h)
```json
// .claude/settings.json (1h)
{
  "mcpServers": {
    "orion-agent-tools": {
      "command": "node",
      "args": ["./mcp-server-orion/dist/index.js"],
      "env": {
        "OPENAI_API_KEY": "${OPENAI_API_KEY}",
        "OS_URL": "http://localhost:9200"
      }
    }
  }
}

// Testing end-to-end (3h)
- Claude Code ‚Üí MCP ‚Üí RAG ‚Üí CI
- Validar custos
- Documentar workflow
```

**Deliverable:** Sistema end-to-end funcionando

### Fase 2: Automa√ß√£o (Semana 2) - 16h

#### Dia 6-7: API Mode (8h)
```python
# orchestrator_api.py (6h)
- Anthropic API client
- GitHub integration
- CI/CD adapter

# .github/workflows/ai-auto.yml (2h)
- Trigger on label
- Call orchestrator_api
- Create PR
```

**Deliverable:** Automa√ß√£o funcionando em CI/CD

#### Dia 8-9: Stack Adapters (4h)
```python
# ai_cli.py expansions (4h)
- Java Maven (1h)
- Java Gradle (1h)
- Go (1h)
- Testing (1h)
```

**Deliverable:** 3 novas stacks suportadas

#### Dia 10: Decision Engine v2 (4h)
```python
# Adaptive learning (4h)
- Embeddings para similarity
- Historical analysis
- Auto-calibration
```

**Deliverable:** Engine adaptativo funcionando

### Fase 3: Produ√ß√£o (Semana 3) - 8h

#### Dia 11-12: Robustez (4h)
```python
# Error handling (2h)
- MCP fallbacks
- State persistence
- Recovery logic

# Cost guards (2h)
- Daily limits
- Alerts
- Monitoring
```

**Deliverable:** Sistema production-ready

#### Dia 13-14: Documenta√ß√£o + Training (4h)
```markdown
# Docs (2h)
- Setup guide
- User manual
- Troubleshooting

# Team training (2h)
- Demo session
- Q&A
- Best practices
```

**Deliverable:** Equipe treinada, docs completos

### Total: 44h √ó $100/h = $4,400 setup
(Mais conservador que $2,000, mas inclui todas as fases)

---

## üéØ M√©tricas de Sucesso

### KPIs Prim√°rios

| M√©trica | Target | Medi√ß√£o |
|---------|--------|---------|
| **Custo/Task** | < $0.80 | Tracking autom√°tico |
| **Success Rate** | > 75% | CI pass first attempt |
| **Claude Max Usage** | > 50% | Tasks via Claude Code |
| **GPT-5 Efficiency** | < 65% tasks | Decision engine stats |
| **Payback** | < 2 meses | Custos vs economia |

### KPIs Secund√°rios

| M√©trica | Target | Medi√ß√£o |
|---------|--------|---------|
| **MCP Uptime** | > 95% | Error rate monitoring |
| **Adaptive Learning** | -20% GPT-5 ap√≥s 3 meses | Historical comparison |
| **Stack Coverage** | 5+ stacks | ai_cli.py adapters |
| **Developer Satisfaction** | > 8/10 | Survey trimestral |

### Dashboard Exemplo

```python
# reports/dashboard.json (atualizado diariamente)
{
  "date": "2025-01-15",
  "tasks": {
    "total": 100,
    "mode": {
      "claude_code": 60,
      "api": 40
    },
    "outcome": {
      "success_first_attempt": 78,
      "success_after_retry": 15,
      "failed": 7
    }
  },
  "costs": {
    "claude_max": 200,  // Mensal fixo
    "gpt5_api": 38.40,  // Di√°rio
    "sonnet_api": 5.40, // Di√°rio (API mode)
    "total_daily": 43.80,
    "projected_monthly": 1314,
    "target": 1556,
    "under_budget": 242
  },
  "decision_engine": {
    "gpt5_calls": 62,
    "correct_decisions": 57,
    "accuracy": 0.92,
    "cost_saved_by_avoiding": 228  // $0.60 √ó 38 tasks
  },
  "stacks": {
    "node": 45,
    "python": 30,
    "java": 15,
    "go": 10
  }
}
```

---

## üöÄ Conclus√£o

### O Que Mudou vs v5.0

| Aspecto | v5.0 Original | v5.1 Final |
|---------|---------------|------------|
| **Distribui√ß√£o tasks** | 80/15/5 (otimista) | 38/42/20 (realista) |
| **Custo/m√™s** | $300 | $1,556 ‚Üí $2,224 |
| **ROI** | 11,700% (irreal) | 248% (conservador) |
| **Automa√ß√£o** | 24/7 assumido | H√≠brido (Claude Code + API) |
| **MCP** | Assume est√°vel | Fallbacks obrigat√≥rios |
| **Decision Engine** | Fixo | Adaptativo com ML |
| **Setup** | $2,000 | $4,400 (inclui tudo) |

### Valida√ß√µes do Feedback Incorporadas

‚úÖ **Distribui√ß√£o realista** - 38% FREE, 62% pago
‚úÖ **Claude Code = manual** - H√≠brido com API para automa√ß√£o
‚úÖ **MCP fallbacks** - Recovery robusto
‚úÖ **Decision adaptativo** - ML learning hist√≥rico
‚úÖ **ROI honesto** - 248% com TCO real
‚úÖ **Arquitetura h√≠brida** - Dev + CI/CD + Produ√ß√£o

### Por Que Este Blueprint Funciona

**1. Realismo**
- Custos conservadores incluem TUDO (setup, manuten√ß√£o, APIs)
- Distribui√ß√£o baseada em dados emp√≠ricos (200 issues)
- ROI honesto com payback verific√°vel

**2. Robustez**
- Fallbacks para MCP, Claude Code, APIs
- State persistence para recovery
- Cost guards para prevenir spikes

**3. Evolutivo**
- Come√ßa simples (MVP semana 1)
- Adiciona automa√ß√£o (semana 2)
- Otimiza com ML (semana 3+)

**4. Stack-Agnostic**
- RAG descobre padr√µes automaticamente
- ai_cli.py adapta comandos
- Nova stack = 20 linhas

**5. Custo-Otimizado**
- Aproveita Claude Max 20x (38% tasks FREE)
- GPT-5 s√≥ quando ROI justifica
- Decision engine aprende e economiza 20-30%

### Pr√≥ximo Passo

**Implementar MVP (Semana 1)**
1. MCP server com 3 tools essenciais
2. Decision engine v1.0 (regras fixas)
3. Testar com 10 tasks reais
4. Medir custos e ajustar

**Gatilho para Fase 2:** Se custo/task < $1.00 e success > 70%, prosseguir com automa√ß√£o.

---

**Aprova√ß√£o para implementar?**
